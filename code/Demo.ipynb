{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2BJzWfE0b9Ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f3b23d-6f1b-4066-ddc6-62c6e2b65f81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SToG'...\n",
            "remote: Enumerating objects: 307, done.\u001b[K\n",
            "remote: Counting objects: 100% (307/307), done.\u001b[K\n",
            "remote: Compressing objects: 100% (244/244), done.\u001b[K\n",
            "remote: Total 307 (delta 95), reused 203 (delta 40), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (307/307), 8.33 MiB | 13.08 MiB/s, done.\n",
            "Resolving deltas: 100% (95/95), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/intsystems/SToG.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd SToG/src/mylib"
      ],
      "metadata": {
        "id": "00bnKQ20XmOk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from SToG.src.mylib.stochastic_gating_complete import STGLayer, STELayer, GumbelLayer, CorrelatedSTGLayer, L1Layer"
      ],
      "metadata": {
        "id": "ZEkA_KIeQesl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from abc import ABC, abstractmethod\n",
        "from sklearn.datasets import load_breast_cancer, load_wine, make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class ModelFeatureSelection(nn.Module):\n",
        "    def __init__(self, model: nn.Sequential, selection_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        for el in selection_layers:\n",
        "            if el[0] < 0 or el[0] > len(list(model)) - 1:\n",
        "                raise ValueError(\"Selection layers must be in the range [1, num_operations]\")\n",
        "\n",
        "        layers = list(model)\n",
        "\n",
        "        for idx, layer in sorted(selection_layers, key=lambda x: x[0], reverse=True):\n",
        "            layers.insert(idx, layer)\n",
        "\n",
        "        self.sel_layer_indices = [el[0] + i for i, el in enumerate(selection_layers)]\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "lUfHe1C-TGyB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "lambda_reg = 0.0001\n",
        "device = 'cpu'\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = ModelFeatureSelection(nn.Sequential(\n",
        "    nn.Linear(30, 10),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(10, 5),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(5, 2)\n",
        "), [(0, STGLayer(30)), (2, STGLayer(10))])\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "    {\"params\": model.layers[i].parameters(), \"lr\": 1e-2} if i in model.sel_layer_indices else {\"params\": model.layers[i].parameters(), \"lr\": 1e-3, \"weight_decay\": 1e-4} for i in range(len(list(model.layers)))\n",
        "])"
      ],
      "metadata": {
        "id": "8XCNwPLwIt3Z"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    predictions = model(X_train)\n",
        "\n",
        "    classification_loss = criterion(predictions, y_train)\n",
        "    regularization_loss = sum(model.layers[i].regularization_loss() for i in model.sel_layer_indices)\n",
        "    total_loss = classification_loss + lambda_reg * regularization_loss\n",
        "\n",
        "    total_loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(\n",
        "        list(model.parameters()),\n",
        "        max_norm=1.0\n",
        "    )\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_predictions = model(X_val)\n",
        "        val_loss = criterion(val_predictions, y_val)\n",
        "        val_acc = (val_predictions.argmax(1) == y_val).float().mean().item() * 100\n",
        "        sel_count = sum(model.layers[i].get_selected_features().sum() for i in model.sel_layer_indices)\n",
        "\n",
        "    return {\n",
        "        'train_loss': total_loss.item(),\n",
        "        'val_loss': val_loss.item(),\n",
        "        'val_acc': val_acc,\n",
        "        'sel_count': sel_count,\n",
        "        'reg_loss': regularization_loss.item()\n",
        "    }"
      ],
      "metadata": {
        "id": "ZOGBYa_g_so9"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(X_train, y_train, X_val, y_val, epochs=300,\n",
        "            patience=50, verbose=False):\n",
        "        \"\"\"\n",
        "        Train the model with early stopping.\n",
        "        \"\"\"\n",
        "        best_val_acc = 0\n",
        "        wait = 0\n",
        "        history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'sel_count': [], 'reg_loss': []}\n",
        "        best_state = {}\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            metrics = train_epoch(X_train, y_train, X_val, y_val)\n",
        "\n",
        "            for key, value in metrics.items():\n",
        "                history[key].append(value)\n",
        "\n",
        "            if metrics['val_acc'] > best_val_acc:\n",
        "                best_val_acc = metrics['val_acc']\n",
        "                wait = 0\n",
        "                best_state = {\n",
        "                    'model': model.state_dict(),\n",
        "                    'epoch': epoch,\n",
        "                    'val_acc': best_val_acc,\n",
        "                    'sel_count': metrics['sel_count']\n",
        "                }\n",
        "            else:\n",
        "                wait += 1\n",
        "\n",
        "            if wait >= patience and epoch >= 100:\n",
        "                if verbose:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "            if verbose and (epoch + 1) % 50 == 0:\n",
        "                print(f\"Epoch {epoch+1}: \"\n",
        "                      f\"val_acc={metrics['val_acc']:.2f}%, \"\n",
        "                      f\"sel={metrics['sel_count']}, \"\n",
        "                      f\"Î»={lambda_reg:.4f}\")\n",
        "\n",
        "        if best_state:\n",
        "            model.load_state_dict(best_state['model'])\n",
        "\n",
        "        return history"
      ],
      "metadata": {
        "id": "8RaoxR42lmlk"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetLoader:\n",
        "    \"\"\"Load and prepare datasets for benchmarking.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_breast_cancer():\n",
        "        \"\"\"Load breast cancer dataset.\"\"\"\n",
        "        data = load_breast_cancer()\n",
        "        return {\n",
        "            'name': 'Breast Cancer',\n",
        "            'X': data.data,\n",
        "            'y': data.target,\n",
        "            'n_important': 10,\n",
        "            'description': 'Binary classification, 30 features'\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def load_wine():\n",
        "        \"\"\"Load wine dataset.\"\"\"\n",
        "        data = load_wine()\n",
        "        return {\n",
        "            'name': 'Wine',\n",
        "            'X': data.data,\n",
        "            'y': data.target,\n",
        "            'n_important': 7,\n",
        "            'description': '3-class classification, 13 features'\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def create_synthetic_high_dim():\n",
        "        \"\"\"Create synthetic high-dimensional dataset (MADELON-like).\"\"\"\n",
        "        X, y = make_classification(\n",
        "            n_samples=600,\n",
        "            n_features=100,\n",
        "            n_informative=5,\n",
        "            n_redundant=10,\n",
        "            n_repeated=0,\n",
        "            n_classes=2,\n",
        "            n_clusters_per_class=2,\n",
        "            flip_y=0.03,\n",
        "            class_sep=1.0,\n",
        "            random_state=42\n",
        "        )\n",
        "        return {\n",
        "            'name': 'Synthetic-HighDim',\n",
        "            'X': X,\n",
        "            'y': y,\n",
        "            'n_important': 5,\n",
        "            'description': 'Binary classification, 100 features, 5 informative'\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def create_synthetic_correlated():\n",
        "        \"\"\"Create synthetic dataset with correlated features.\"\"\"\n",
        "        np.random.seed(42)\n",
        "        n_samples = 500\n",
        "        n_informative = 5\n",
        "        n_total = 50\n",
        "\n",
        "        X_inform = np.random.randn(n_samples, n_informative)\n",
        "\n",
        "        X_redundant = []\n",
        "        for i in range(n_informative):\n",
        "            for _ in range(2):\n",
        "                noise = np.random.randn(n_samples) * 0.1\n",
        "                X_redundant.append(X_inform[:, i] + noise)\n",
        "        X_redundant = np.column_stack(X_redundant)\n",
        "\n",
        "        n_noise = n_total - n_informative - X_redundant.shape[1]\n",
        "        X_noise = np.random.randn(n_samples, n_noise)\n",
        "\n",
        "        X = np.column_stack([X_inform, X_redundant, X_noise])\n",
        "\n",
        "        y = (X_inform[:, 0] + X_inform[:, 1] * X_inform[:, 2] > 0).astype(int)\n",
        "\n",
        "        return {\n",
        "            'name': 'Synthetic-Correlated',\n",
        "            'X': X,\n",
        "            'y': y,\n",
        "            'n_important': n_informative,\n",
        "            'description': f'Binary classification, {n_total} features, {n_informative} informative with correlated copies'\n",
        "        }"
      ],
      "metadata": {
        "id": "DOZYNWwsmK80"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = DatasetLoader().load_breast_cancer()\n",
        "random_state = 42\n",
        "\n",
        "X, y = dataset['X'], dataset['y']\n",
        "n_features = X.shape[1]\n",
        "n_classes = len(np.unique(y))\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=random_state, stratify=y\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.2, random_state=random_state, stratify=y_temp\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train_t = torch.FloatTensor(X_train).to(device)\n",
        "y_train_t = torch.LongTensor(y_train).to(device)\n",
        "X_val_t = torch.FloatTensor(X_val).to(device)\n",
        "y_val_t = torch.LongTensor(y_val).to(device)\n",
        "X_test_t = torch.FloatTensor(X_test).to(device)\n",
        "y_test_t = torch.LongTensor(y_test).to(device)"
      ],
      "metadata": {
        "id": "c9fkzr4wmXQl"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit(X_train_t, y_train_t, X_val_t, y_val_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CMMawIUm8Gc",
        "outputId": "51e416d7-e767-49ad-de5f-c4006d829223"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train_loss': [0.9246473908424377,\n",
              "  0.9194371104240417,\n",
              "  0.9222840070724487,\n",
              "  0.92277592420578,\n",
              "  0.920691192150116,\n",
              "  0.9165613651275635,\n",
              "  0.915504515171051,\n",
              "  0.92184978723526,\n",
              "  0.9157575964927673,\n",
              "  0.9109681844711304,\n",
              "  0.9113531708717346,\n",
              "  0.9122119545936584,\n",
              "  0.9148720502853394,\n",
              "  0.9090137481689453,\n",
              "  0.9081457257270813,\n",
              "  0.9051733016967773,\n",
              "  0.9075988531112671,\n",
              "  0.9025763869285583,\n",
              "  0.9009677171707153,\n",
              "  0.9006478190422058,\n",
              "  0.9036339521408081,\n",
              "  0.9004407525062561,\n",
              "  0.8988052010536194,\n",
              "  0.901374876499176,\n",
              "  0.9007357358932495,\n",
              "  0.8943776488304138,\n",
              "  0.8972218036651611,\n",
              "  0.8935827016830444,\n",
              "  0.892792284488678,\n",
              "  0.8854230046272278,\n",
              "  0.8862276077270508,\n",
              "  0.8820794820785522,\n",
              "  0.8795429468154907,\n",
              "  0.8821713924407959,\n",
              "  0.8828898668289185,\n",
              "  0.8850051760673523,\n",
              "  0.8800486922264099,\n",
              "  0.8800867199897766,\n",
              "  0.8775691390037537,\n",
              "  0.8737404346466064,\n",
              "  0.8741771578788757,\n",
              "  0.8746575713157654,\n",
              "  0.8701006770133972,\n",
              "  0.8713622689247131,\n",
              "  0.8634679317474365,\n",
              "  0.8663108944892883,\n",
              "  0.857269287109375,\n",
              "  0.86004638671875,\n",
              "  0.8635851144790649,\n",
              "  0.8623873591423035,\n",
              "  0.8607437014579773,\n",
              "  0.8586606383323669,\n",
              "  0.8657283186912537,\n",
              "  0.861455500125885,\n",
              "  0.8574153780937195,\n",
              "  0.8514953255653381,\n",
              "  0.8381645083427429,\n",
              "  0.8492959141731262,\n",
              "  0.8480125069618225,\n",
              "  0.8549498319625854,\n",
              "  0.8411214351654053,\n",
              "  0.8291898369789124,\n",
              "  0.8365664482116699,\n",
              "  0.8516928553581238,\n",
              "  0.8334222435951233,\n",
              "  0.8280479907989502,\n",
              "  0.8261218667030334,\n",
              "  0.8401466608047485,\n",
              "  0.7984084486961365,\n",
              "  0.8384661078453064,\n",
              "  0.8154635429382324,\n",
              "  0.8136826753616333,\n",
              "  0.8156962394714355,\n",
              "  0.7960983514785767,\n",
              "  0.7951971292495728,\n",
              "  0.8090459704399109,\n",
              "  0.7768029570579529,\n",
              "  0.7832092046737671,\n",
              "  0.8023360967636108,\n",
              "  0.7697252631187439,\n",
              "  0.7788985371589661,\n",
              "  0.7682685256004333,\n",
              "  0.7847612500190735,\n",
              "  0.7621967792510986,\n",
              "  0.7563058733940125,\n",
              "  0.7912583947181702,\n",
              "  0.7780084013938904,\n",
              "  0.7183899283409119,\n",
              "  0.7736282348632812,\n",
              "  0.730519711971283,\n",
              "  0.7306099534034729,\n",
              "  0.7626441717147827,\n",
              "  0.7191982865333557,\n",
              "  0.7229963541030884,\n",
              "  0.7161937355995178,\n",
              "  0.7273719310760498,\n",
              "  0.7014445066452026,\n",
              "  0.6693040132522583,\n",
              "  0.7011340260505676,\n",
              "  0.6750739216804504,\n",
              "  0.6263810396194458],\n",
              " 'val_loss': [0.9214890003204346,\n",
              "  0.9203293323516846,\n",
              "  0.9191725850105286,\n",
              "  0.9180229902267456,\n",
              "  0.9168732762336731,\n",
              "  0.9157288074493408,\n",
              "  0.9145835041999817,\n",
              "  0.9134341478347778,\n",
              "  0.9122835993766785,\n",
              "  0.9111335873603821,\n",
              "  0.9099819660186768,\n",
              "  0.9088239669799805,\n",
              "  0.9076674580574036,\n",
              "  0.9065119028091431,\n",
              "  0.9053571820259094,\n",
              "  0.9042062163352966,\n",
              "  0.903059720993042,\n",
              "  0.9019111394882202,\n",
              "  0.9007594585418701,\n",
              "  0.899606466293335,\n",
              "  0.898450493812561,\n",
              "  0.8972945809364319,\n",
              "  0.8961360454559326,\n",
              "  0.8949821591377258,\n",
              "  0.8938323855400085,\n",
              "  0.892682671546936,\n",
              "  0.8915395140647888,\n",
              "  0.8903982043266296,\n",
              "  0.8892475366592407,\n",
              "  0.8880806565284729,\n",
              "  0.8869010806083679,\n",
              "  0.885701596736908,\n",
              "  0.8844794034957886,\n",
              "  0.8832409977912903,\n",
              "  0.8819928765296936,\n",
              "  0.8807492256164551,\n",
              "  0.8795053958892822,\n",
              "  0.8782534003257751,\n",
              "  0.8769848346710205,\n",
              "  0.8756982088088989,\n",
              "  0.8743977546691895,\n",
              "  0.8730958104133606,\n",
              "  0.8717812299728394,\n",
              "  0.870451807975769,\n",
              "  0.8690772652626038,\n",
              "  0.867667555809021,\n",
              "  0.8662375211715698,\n",
              "  0.8647842407226562,\n",
              "  0.8633094429969788,\n",
              "  0.8618122935295105,\n",
              "  0.8602752089500427,\n",
              "  0.858689546585083,\n",
              "  0.8571184873580933,\n",
              "  0.8555412888526917,\n",
              "  0.8539488315582275,\n",
              "  0.8522944450378418,\n",
              "  0.8505401015281677,\n",
              "  0.8487502932548523,\n",
              "  0.8468776941299438,\n",
              "  0.8449857831001282,\n",
              "  0.8430246114730835,\n",
              "  0.8409819006919861,\n",
              "  0.8388503193855286,\n",
              "  0.8366488218307495,\n",
              "  0.834306001663208,\n",
              "  0.8318230509757996,\n",
              "  0.8292407393455505,\n",
              "  0.8265588879585266,\n",
              "  0.8236458897590637,\n",
              "  0.8207083940505981,\n",
              "  0.8175942301750183,\n",
              "  0.8143414258956909,\n",
              "  0.810871422290802,\n",
              "  0.8072105646133423,\n",
              "  0.8033748269081116,\n",
              "  0.7992725372314453,\n",
              "  0.7949445247650146,\n",
              "  0.790587306022644,\n",
              "  0.7861474752426147,\n",
              "  0.7815525531768799,\n",
              "  0.7769078016281128,\n",
              "  0.772065281867981,\n",
              "  0.767085075378418,\n",
              "  0.7621020078659058,\n",
              "  0.7569937109947205,\n",
              "  0.7517074942588806,\n",
              "  0.7463341951370239,\n",
              "  0.7407447695732117,\n",
              "  0.7348812222480774,\n",
              "  0.7287591695785522,\n",
              "  0.7223480343818665,\n",
              "  0.7159252166748047,\n",
              "  0.7092017531394958,\n",
              "  0.7021884918212891,\n",
              "  0.6950171589851379,\n",
              "  0.6876285076141357,\n",
              "  0.6802178621292114,\n",
              "  0.6726018190383911,\n",
              "  0.6649179458618164,\n",
              "  0.6569846868515015,\n",
              "  0.6490825414657593],\n",
              " 'val_acc': [37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287,\n",
              "  37.36263811588287],\n",
              " 'sel_count': [np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(9),\n",
              "  np.int64(11),\n",
              "  np.int64(10),\n",
              "  np.int64(9),\n",
              "  np.int64(9),\n",
              "  np.int64(9),\n",
              "  np.int64(10),\n",
              "  np.int64(11),\n",
              "  np.int64(13),\n",
              "  np.int64(13),\n",
              "  np.int64(13),\n",
              "  np.int64(14),\n",
              "  np.int64(14),\n",
              "  np.int64(14),\n",
              "  np.int64(14),\n",
              "  np.int64(14),\n",
              "  np.int64(15),\n",
              "  np.int64(15),\n",
              "  np.int64(15),\n",
              "  np.int64(14),\n",
              "  np.int64(14),\n",
              "  np.int64(14),\n",
              "  np.int64(14),\n",
              "  np.int64(14),\n",
              "  np.int64(13),\n",
              "  np.int64(13),\n",
              "  np.int64(13),\n",
              "  np.int64(14),\n",
              "  np.int64(14),\n",
              "  np.int64(14),\n",
              "  np.int64(14),\n",
              "  np.int64(14),\n",
              "  np.int64(14),\n",
              "  np.int64(14),\n",
              "  np.int64(14),\n",
              "  np.int64(14),\n",
              "  np.int64(14),\n",
              "  np.int64(14),\n",
              "  np.int64(14),\n",
              "  np.int64(14),\n",
              "  np.int64(16),\n",
              "  np.int64(16),\n",
              "  np.int64(16),\n",
              "  np.int64(17),\n",
              "  np.int64(17),\n",
              "  np.int64(17),\n",
              "  np.int64(17),\n",
              "  np.int64(17),\n",
              "  np.int64(17),\n",
              "  np.int64(17),\n",
              "  np.int64(17),\n",
              "  np.int64(17),\n",
              "  np.int64(17),\n",
              "  np.int64(17),\n",
              "  np.int64(17),\n",
              "  np.int64(18),\n",
              "  np.int64(18),\n",
              "  np.int64(18),\n",
              "  np.int64(19),\n",
              "  np.int64(19),\n",
              "  np.int64(19),\n",
              "  np.int64(19),\n",
              "  np.int64(19),\n",
              "  np.int64(19),\n",
              "  np.int64(20),\n",
              "  np.int64(21),\n",
              "  np.int64(21),\n",
              "  np.int64(21),\n",
              "  np.int64(21),\n",
              "  np.int64(21),\n",
              "  np.int64(21),\n",
              "  np.int64(21),\n",
              "  np.int64(22),\n",
              "  np.int64(22),\n",
              "  np.int64(22),\n",
              "  np.int64(22),\n",
              "  np.int64(22),\n",
              "  np.int64(22),\n",
              "  np.int64(22),\n",
              "  np.int64(22),\n",
              "  np.int64(22),\n",
              "  np.int64(22),\n",
              "  np.int64(22),\n",
              "  np.int64(23),\n",
              "  np.int64(23),\n",
              "  np.int64(23),\n",
              "  np.int64(24),\n",
              "  np.int64(24),\n",
              "  np.int64(24),\n",
              "  np.int64(24),\n",
              "  np.int64(24),\n",
              "  np.int64(24),\n",
              "  np.int64(24),\n",
              "  np.int64(24),\n",
              "  np.int64(24),\n",
              "  np.int64(24),\n",
              "  np.int64(24),\n",
              "  np.int64(24),\n",
              "  np.int64(24)],\n",
              " 'reg_loss': [20.0,\n",
              "  19.69686508178711,\n",
              "  19.55718231201172,\n",
              "  19.40117073059082,\n",
              "  19.330303192138672,\n",
              "  19.269893646240234,\n",
              "  19.203601837158203,\n",
              "  19.14087677001953,\n",
              "  19.083419799804688,\n",
              "  19.037029266357422,\n",
              "  19.01114273071289,\n",
              "  18.992088317871094,\n",
              "  18.97438621520996,\n",
              "  18.951644897460938,\n",
              "  18.926443099975586,\n",
              "  18.901063919067383,\n",
              "  18.868648529052734,\n",
              "  18.848947525024414,\n",
              "  18.82954978942871,\n",
              "  18.81902313232422,\n",
              "  18.811264038085938,\n",
              "  18.795886993408203,\n",
              "  18.778146743774414,\n",
              "  18.758514404296875,\n",
              "  18.73648452758789,\n",
              "  18.71062469482422,\n",
              "  18.69086456298828,\n",
              "  18.669208526611328,\n",
              "  18.647432327270508,\n",
              "  18.635814666748047,\n",
              "  18.630874633789062,\n",
              "  18.638076782226562,\n",
              "  18.656005859375,\n",
              "  18.683494567871094,\n",
              "  18.714622497558594,\n",
              "  18.746923446655273,\n",
              "  18.775890350341797,\n",
              "  18.811458587646484,\n",
              "  18.8455867767334,\n",
              "  18.885000228881836,\n",
              "  18.931501388549805,\n",
              "  18.983386993408203,\n",
              "  19.037527084350586,\n",
              "  19.092676162719727,\n",
              "  19.148099899291992,\n",
              "  19.210548400878906,\n",
              "  19.28131866455078,\n",
              "  19.35545539855957,\n",
              "  19.429317474365234,\n",
              "  19.49941062927246,\n",
              "  19.56810760498047,\n",
              "  19.63899803161621,\n",
              "  19.71515464782715,\n",
              "  19.782062530517578,\n",
              "  19.849346160888672,\n",
              "  19.914918899536133,\n",
              "  19.989669799804688,\n",
              "  20.08391571044922,\n",
              "  20.182950973510742,\n",
              "  20.27804946899414,\n",
              "  20.369564056396484,\n",
              "  20.471174240112305,\n",
              "  20.578433990478516,\n",
              "  20.686338424682617,\n",
              "  20.78643226623535,\n",
              "  20.885391235351562,\n",
              "  20.988750457763672,\n",
              "  21.09166717529297,\n",
              "  21.190963745117188,\n",
              "  21.305227279663086,\n",
              "  21.412097930908203,\n",
              "  21.518606185913086,\n",
              "  21.62754249572754,\n",
              "  21.74022102355957,\n",
              "  21.857715606689453,\n",
              "  21.97533416748047,\n",
              "  22.090730667114258,\n",
              "  22.21375846862793,\n",
              "  22.329587936401367,\n",
              "  22.439952850341797,\n",
              "  22.550399780273438,\n",
              "  22.661407470703125,\n",
              "  22.777698516845703,\n",
              "  22.89919090270996,\n",
              "  23.020553588867188,\n",
              "  23.14502716064453,\n",
              "  23.273914337158203,\n",
              "  23.395042419433594,\n",
              "  23.51742935180664,\n",
              "  23.6430721282959,\n",
              "  23.76207733154297,\n",
              "  23.8889217376709,\n",
              "  24.007408142089844,\n",
              "  24.13486099243164,\n",
              "  24.26491928100586,\n",
              "  24.39385223388672,\n",
              "  24.52079200744629,\n",
              "  24.63690757751465,\n",
              "  24.74893569946289,\n",
              "  24.857728958129883,\n",
              "  24.96714973449707]}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}